# -*- coding: utf-8 -*-
"""novel_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-A28Ujdy2BwZv5s6gsQC6SZsg5bq3_Lk
"""

import re
import os
import numpy as np
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Embedding, LSTM, Dense, Dropout
from keras.preprocessing.text import Tokenizer
from keras.models import Sequential
import keras.utils as ku
import nltk
nltk.download()


text = open('924-0.txt', 'rb').read().decode(encoding='utf-8-sig').lower()
print(text[:50])
text = re.sub(r'[#$@\[\]\\/_+=\(\):;<>\*\s+]', ' ', text)
sentences = nltk.sent_tokenize(text)

# sentences[:20]

tokenizer = Tokenizer()
tokenizer.fit_on_texts(sentences)

words_length = len(tokenizer.word_index) + 1
print(words_length)


sequences = []
for sentence in sentences:
    words = tokenizer.texts_to_sequences([sentence])[0]
    for i in range(1, len(words)):
        sequence = words[:i+1]
        sequences.append(sequence)
sequences[:10]

max_length_sequence = max([len(i) for i in sequences])
max_length_sequence

sequences = np.array(pad_sequences(
    sequences, maxlen=max_length_sequence, padding='pre'))
sequences[:2]

X = sequences[:, :-1]
Y = ku.to_categorical(sequences[:, -1], num_classes=words_length)
#X = X.reshape(X.shape[0], max_length_sequence - 1, 1)
print(X.shape)
print(X[:1])
print(len(Y[1]))
input_len = max_length_sequence - 1
print(words_length)

model = Sequential()
input_len = max_length_sequence - 1

model.add(Embedding(words_length, 10, input_length=input_len))
model.add(LSTM(400, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(400))
model.add(Dropout(0.2))
model.add(Dense(words_length, activation='softmax'))

model.compile(loss='categorical_crossentropy',
              optimizer='adam', metrics=['accuracy'])

model.summary()

model.fit(X, Y, epochs=20, verbose=5)

model.save('novel_model.model')

inp = "he went there alone looking for a fall"

for i in range(30):
    words = tokenizer.texts_to_sequences([inp])[0]
    sequence = pad_sequences(
        [words], maxlen=max_length_sequence - 1, padding='pre')
    #sequence = sequence.reshape(sequence.shape[0], sequence.shape[-1], 1)
    predicted = model.predict_classes(sequence, verbose=0)
    for prediction, index in tokenizer.word_index.items():
        output_word = ""
        if index == predicted:
            output_word = prediction
            break
    inp += " " + output_word
print(inp)
